---
title: "hoge"
author: 
date: "2016年7月11日"
output: html_document
---

#概要
トピックモデル
Windowsで実施

#データの準備と概観
```{r warning=FALSE}
#準備----
setwd("path")

library(readr)
library(dplyr)
library(stringr)
library(RMeCab)
library(lda)
library(ggplot2)
library(knitr)


##読み込み
dat <- read_csv("hoge.csv", locale = locale(encoding = "cp932"))
```

まずはどの商品に多くコメントされているかを調べる
```{r}
hoge_all <- dat %>%
  filter(str_detect(dat$name, "hoge") == "TRUE") %>%
  group_by(unique_name) %>%
  summarise(n = n()) %>%
  arrange(desc(n))
kable(head(hoge_all, 10)) #先頭10行を表示
```

次にコメントデータの作成。
```{r}
hogehoge <- dat %>%
  filter(str_detect(dat$name, "hoge") == "TRUE") %>%
  select(comment)
kable(head(hoge, 10))
```


##多く使われている語を使用する
```{r}
##RMeCabFreqのごみ掃除用の関数を作成----
trush_delete <- function (x) {
  x <- dplyr::filter(x, Info1 == c("名詞", "形容詞"), 
              Info2 != "非自立",
              #stringr::str_length(x$Term) > 1, #文字数2以上
              stringr::str_detect(x$Term, '[:punct:]') == 'FALSE', 
              str_detect(x$Term, '[A-z0-9]') == 'FALSE', #アルファベットと数字
              str_detect(x$Term, '[Α-ω]') == 'FALSE', #ギリシャ文字
              str_detect(x$Term, '[А-я]') == 'FALSE', #キリル文字
              str_detect(x$Term, '[ｦ-ﾝ]') == 'FALSE' #半角カタカナ
  )
}

#形態素解析(1語)
uni_hoge <- RMeCabFreq("hoge.csv") %>%
  trush_delete() %>%
  arrange(desc(Freq))
##write.csv(uni_hoge, "hoge形態素.csv", row.names = FALSE)
kable(head(uni_hoge, 10))

#バイグラム(2語の組み合わせ)
bi_hoge <- docDF(target = "hoge.csv", type = 1, N = 2, Genkei = 1) %>%
  arrange(desc(hoge.csv))
##write.csv(bi_hoge, "バイグラム.csv", row.names = FALSE)
kable(head(bi_hoge, 10))

#トリグラム(3語の組み合わせ)
tri_hoge <- docDF(target = "hoge.csv", type = 1, N = 3, Genkei = 1) %>%
  arrange(desc(hoge.csv))
##write.csv(tri_hoge, "トリグラム.csv", row.names = FALSE)
kable(head(tri_hoge, 10))
```

##トピックモデルでの評価
今回は潜在ディリクレ過程(Latent Dirichlet Allocation, LDA)を用いた。

###LDAとは
1つの文書が複数のトピックから成ることを仮定した言語モデルの一種。詳細は以下の画像および引用先を参照。

![](http://f.st-hatena.com/images/fotolife/n/ni66ling/20150504/20150504032913_original.png?1430677772)

（引用：[LDAの各変数の意味と幾何的解釈について](http://ni66ling.hatenadiary.jp/entry/2015/05/04/163958)）

$\alpha$と$\beta$(プログラム上は$\eta$になっている)はハイパーパラメータとして与える。

今回は1コメント1文書として分析を行った。

###LDAの実行
```{r}
#トピックモデル----
##形態素解析
hoge_txt <- RMeCabText("hoge.csv")

##空のデータフレームを作成
hoge_doc <- NULL

##要素の一番最初（文中に出現する語形）のみ取り出す
for (i in 1:length(hoge_txt)) {
  if (hoge_txt[[i]][2] %in% c("名詞", "形容詞")) {
    hoge_doc <- c(hoge_doc, paste(hoge_txt[[i]][1], sep = "", collapse = " "))
  }
}

##コーパスの作成
hoge_lex <- lexicalize(hoge_doc, lower = TRUE)

##ギッブスサンプリングで解析
k <- 10 #トピック数

set.seed(123)
result <- lda.collapsed.gibbs.sampler(hoge_lex$documents, 
                                      k,
                                      hoge_lex$vocab,
                                      burnin = 500,
                                      1000,  # 繰り返し数
                                      10, # トピックの生起パラメータalpha
                                      1, # ディリクレ分布のスカラパラメータeta
                                      compute.log.likelihood=TRUE)
## 結果を表示 
summary(result)
#head(result$log.likelihoods)

log_likelihoods <- t(result$log.likelihoods) %>%
  as_data_frame()

Iteration <- 1:nrow(log_likelihoods)

##対数尤度をプロット。収束の確認。
ggplot(log_likelihoods, aes(x = Iteration, y = Log_Likelihoods)) +
  geom_line(aes(y = V1, colour = "V1")) +
  geom_line(aes(y = V2, colour = "V2")) +
  scale_color_hue(name = "種類", labels = c(V1 = "Full", V2 ="Obs") )
```

赤が事前分布を含めた対数尤度、青が観測されたものからのみの対数尤度。
青いほうに注目すればよく、対数尤度が収束していることを確認。

```{r}
## 各トピックにおける上位5位の単語の行列
##(各トピックで現れる確率が高い単語)
top.words <- top.topic.words(result$topics, 5, by.score = TRUE) %>%
  as_data_frame()
colnames(top.words) <- c("Topic1", "Topic2", "Topic3", "Topic4", "Topic5", "Topic6", "Topic7", "Topic8", "Topic9", "Topic10")
kable(top.words)
```

上記は、hogeのコメントにお現れるトピックと、そのトピック中に現れる上位５単語をあらわしたもので、ここから各トピックを解釈し、軸の作成に役立てる。
これらのトピックは解釈可能なものと不可能なものがあるが、解釈可能なものは以下のように解釈できる。
